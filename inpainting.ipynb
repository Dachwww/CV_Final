{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for **\"Inpainting\"** figures $6$, $8$ and 7 (top) from the main paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*Uncomment if running on colab* \\nSet Runtime -> Change runtime type -> Under Hardware Accelerator select GPU in Google Colab \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "*Uncomment if running on colab* \n",
    "Set Runtime -> Change runtime type -> Under Hardware Accelerator select GPU in Google Colab \n",
    "\"\"\"\n",
    "# !git clone https://github.com/DmitryUlyanov/deep-image-prior\n",
    "# !mv deep-image-prior/* ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from models.resnet import ResNet\n",
    "from models.unet import UNet\n",
    "from models.skip import skip\n",
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "from utils.inpainting_utils import *\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark =True\n",
    "dtype = torch.cuda.FloatTensor\n",
    "\n",
    "PLOT = True\n",
    "imsize = -1\n",
    "dim_div_by = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fig 6\n",
    "# img_path  = 'data/inpainting/vase.png'\n",
    "# mask_path = 'data/inpainting/vase_mask.png'\n",
    "\n",
    "## Fig 8\n",
    "# img_path  = 'data/inpainting/library.png'\n",
    "# mask_path = 'data/inpainting/library_mask.png'\n",
    "\n",
    "## Fig 7 (top)\n",
    "img_path  = 'data/inpainting/kate.png'\n",
    "mask_path = 'data/inpainting/kate_mask.png'\n",
    "\n",
    "# Another text inpainting example\n",
    "# img_path  = 'data/inpainting/peppers.png'\n",
    "# mask_path = 'data/inpainting/peppers_mask.png'\n",
    "\n",
    "NET_TYPE = 'skip_depth6' # one of skip_depth4|skip_depth2|UNET|ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pil, img_np = get_image(img_path, imsize)\n",
    "img_mask_pil, img_mask_np = get_image(mask_path, imsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask_pil = crop_image(img_mask_pil, dim_div_by)\n",
    "img_pil      = crop_image(img_pil,      dim_div_by)\n",
    "\n",
    "img_np      = pil_to_np(img_pil)\n",
    "img_mask_np = pil_to_np(img_mask_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb 单元格 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m img_mask_var \u001b[39m=\u001b[39m np_to_torch(img_mask_np)\u001b[39m.\u001b[39mtype(dtype)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plot_image_grid([img_np, img_mask_np, img_mask_np\u001b[39m*\u001b[39mimg_np], \u001b[39m3\u001b[39m,\u001b[39m11\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "img_mask_var = np_to_torch(img_mask_np).type(dtype)\n",
    "\n",
    "plot_image_grid([img_np, img_mask_np, img_mask_np*img_np], 3,11);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'reflection' # 'zero'\n",
    "OPT_OVER = 'net'\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb 单元格 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     figsize \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     reg_noise_std \u001b[39m=\u001b[39m \u001b[39m0.03\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     net \u001b[39m=\u001b[39m skip(input_depth, img_np\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                num_channels_down \u001b[39m=\u001b[39m [\u001b[39m128\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                num_channels_up \u001b[39m=\u001b[39m   [\u001b[39m128\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                num_channels_skip \u001b[39m=\u001b[39m    [\u001b[39m128\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m5\u001b[39m,  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                filter_size_up \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, filter_size_down \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                upsample_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m, filter_skip_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                need_sigmoid\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, need_bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pad\u001b[39m=\u001b[39mpad, act_fun\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLeakyReLU\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtype(dtype)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mlibrary.png\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m img_path:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     INPUT \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:981\u001b[0m, in \u001b[0;36mModule.type\u001b[0;34m(self, dst_type)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m: T, dst_type: Union[dtype, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    970\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \n\u001b[1;32m    972\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mtype(dst_type))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:981\u001b[0m, in \u001b[0;36mModule.type.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m: T, dst_type: Union[dtype, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    970\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Casts all parameters and buffers to :attr:`dst_type`.\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \n\u001b[1;32m    972\u001b[0m \u001b[39m    .. note::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mtype(dst_type))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "if 'vase.png' in img_path:\n",
    "    INPUT = 'meshgrid'\n",
    "    input_depth = 2\n",
    "    LR = 0.01 \n",
    "    num_iter = 5001\n",
    "    param_noise = False\n",
    "    show_every = 50\n",
    "    figsize = 5\n",
    "    reg_noise_std = 0.03\n",
    "    \n",
    "    net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [128] * 5,\n",
    "               num_channels_up   = [128] * 5,\n",
    "               num_channels_skip = [0] * 5,  \n",
    "               upsample_mode='nearest', filter_skip_size=1, filter_size_up=3, filter_size_down=3,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "    \n",
    "elif ('kate.png' in img_path) or ('peppers.png' in img_path):\n",
    "    # Same params and net as in super-resolution and denoising\n",
    "    INPUT = 'noise'\n",
    "    input_depth = 32\n",
    "    LR = 0.01 \n",
    "    num_iter = 6001\n",
    "    param_noise = False\n",
    "    show_every = 50\n",
    "    figsize = 5\n",
    "    reg_noise_std = 0.03\n",
    "    \n",
    "    net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [128] * 5,\n",
    "               num_channels_up =   [128] * 5,\n",
    "               num_channels_skip =    [128] * 5,  \n",
    "               filter_size_up = 3, filter_size_down = 3, \n",
    "               upsample_mode='nearest', filter_skip_size=1,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "    \n",
    "elif 'library.png' in img_path:\n",
    "    \n",
    "    INPUT = 'noise'\n",
    "    input_depth = 1\n",
    "    \n",
    "    num_iter = 3001\n",
    "    show_every = 50\n",
    "    figsize = 8\n",
    "    reg_noise_std = 0.00\n",
    "    param_noise = True\n",
    "    \n",
    "    if 'skip' in NET_TYPE:\n",
    "        \n",
    "        depth = int(NET_TYPE[-1])\n",
    "        net = skip(input_depth, img_np.shape[0], \n",
    "               num_channels_down = [16, 32, 64, 128, 128, 128][:depth],\n",
    "               num_channels_up =   [16, 32, 64, 128, 128, 128][:depth],\n",
    "               num_channels_skip =    [0, 0, 0, 0, 0, 0][:depth],  \n",
    "               filter_size_up = 3,filter_size_down = 5,  filter_skip_size=1,\n",
    "               upsample_mode='nearest', # downsample_mode='avg',\n",
    "               need1x1_up=False,\n",
    "               need_sigmoid=True, need_bias=True, pad=pad, act_fun='LeakyReLU').type(dtype)\n",
    "        \n",
    "        LR = 0.01 \n",
    "        \n",
    "    elif NET_TYPE == 'UNET':\n",
    "        \n",
    "        net = UNet(num_input_channels=input_depth, num_output_channels=3, \n",
    "                   feature_scale=8, more_layers=1, \n",
    "                   concat_x=False, upsample_mode='deconv', \n",
    "                   pad='zero', norm_layer=torch.nn.InstanceNorm2d, need_sigmoid=True, need_bias=True)\n",
    "        \n",
    "        LR = 0.001\n",
    "        param_noise = False\n",
    "        \n",
    "    elif NET_TYPE == 'ResNet':\n",
    "        \n",
    "        net = ResNet(input_depth, img_np.shape[0], 8, 32, need_sigmoid=True, act_fun='LeakyReLU')\n",
    "        \n",
    "        LR = 0.001\n",
    "        param_noise = False\n",
    "        \n",
    "    else:\n",
    "        assert False\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "net = net.type(dtype)\n",
    "net_input = get_noise(input_depth, INPUT, img_np.shape[1:]).type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb 单元格 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Compute number of parameters\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m s  \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(np\u001b[39m.\u001b[39mprod(\u001b[39mlist\u001b[39m(p\u001b[39m.\u001b[39msize())) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mNumber of params: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m s)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lml/Documents/GitHub/CV_Final/inpainting.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Loss\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute number of parameters\n",
    "s  = sum(np.prod(list(p.size())) for p in net.parameters())\n",
    "print ('Number of params: %d' % s)\n",
    "\n",
    "# Loss\n",
    "mse = torch.nn.MSELoss().type(dtype)\n",
    "\n",
    "img_var = np_to_torch(img_np).type(dtype)\n",
    "mask_var = np_to_torch(img_mask_np).type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "def closure():\n",
    "    \n",
    "    global i\n",
    "    \n",
    "    if param_noise:\n",
    "        for n in [x for x in net.parameters() if len(x.size()) == 4]:\n",
    "            n = n + n.detach().clone().normal_() * n.std() / 50\n",
    "    \n",
    "    net_input = net_input_saved\n",
    "    if reg_noise_std > 0:\n",
    "        net_input = net_input_saved + (noise.normal_() * reg_noise_std)\n",
    "        \n",
    "        \n",
    "    out = net(net_input)\n",
    "   \n",
    "    total_loss = mse(out * mask_var, img_var * mask_var)\n",
    "    total_loss.backward()\n",
    "        \n",
    "    print ('Iteration %05d    Loss %f' % (i, total_loss.item()), '\\r', end='')\n",
    "    if  PLOT and i % show_every == 0:\n",
    "        out_np = torch_to_np(out)\n",
    "        plot_image_grid([np.clip(out_np, 0, 1)], factor=figsize, nrow=1)\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "net_input_saved = net_input.detach().clone()\n",
    "noise = net_input.detach().clone()\n",
    "\n",
    "p = get_params(OPT_OVER, net, net_input)\n",
    "optimize(OPTIMIZER, p, closure, LR, num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_np = torch_to_np(net(net_input))\n",
    "plot_image_grid([out_np], factor=5);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
